{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b98195",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What are Sequence-to-sequence models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ddecc",
   "metadata": {},
   "source": [
    "Sequence to Sequence (often abbreviated to seq2seq) models is a special class of Recurrent Neural Network architectures that we typically use (but not restricted) to solve complex Language problems like Machine Translation, Question Answering, creating Chatbots, Text Summarization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca91cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What are the Problem with Vanilla RNNs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7e9d5",
   "metadata": {},
   "source": [
    "However, RNNs suffer from the problem of vanishing gradients, which hampers learning of long data sequences. The gradients carry information used in the RNN parameter update and when the gradient becomes smaller and smaller, the parameter updates become insignificant which means no real learning is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abb5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What is Gradient clipping?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a50f8",
   "metadata": {},
   "source": [
    "Gradient clipping is a technique to prevent exploding gradients in very deep networks, usually in recurrent neural networks. ... With gradient clipping, pre-determined gradient threshold be introduced, and then gradients norms that exceed this threshold are scaled down to match the norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad90e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Explain Attention mechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11493634",
   "metadata": {},
   "source": [
    "\n",
    "Attention takes two sentences, turns them into a matrix where the words of one sentence form the columns, and the words of another sentence form the rows, and then it makes matches, identifying relevant context. This is very useful in machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e248c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Explain Conditional random fields (CRFs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f94b240",
   "metadata": {},
   "source": [
    "Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighboring\" samples, a CRF can take context into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Explain self-attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5688db9",
   "metadata": {},
   "source": [
    "Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What is Bahdanau Attention?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80acb0e",
   "metadata": {},
   "source": [
    "Bahdanau attention mechanism\n",
    "proposed an attention mechanism that learns to align and translate jointly. It is also known as Additive attention as it performs a linear combination of encoder states and the decoder states. ... It helps to pay attention to the most relevant information in the source sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a2d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. What is a Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c507b",
   "metadata": {},
   "source": [
    "Gentle Introduction to Statistical Language Modeling and Neural Language Models\n",
    "by Jason Brownlee on November 1, 2017 in Deep Learning for Natural Language Processing\n",
    "Tweet  Share\n",
    "Last Updated on August 7, 2019\n",
    "\n",
    "Language modeling is central to many important natural language processing tasks.\n",
    "\n",
    "Recently, neural-network-based language models have demonstrated better performance than classical methods both standalone and as part of more challenging natural language processing tasks.\n",
    "\n",
    "In this post, you will discover language modeling for natural language processing.\n",
    "\n",
    "After reading this post, you will know:\n",
    "\n",
    "Why language modeling is critical to addressing tasks in natural language processing.\n",
    "What a language model is and some examples of where they are used.\n",
    "How neural networks can be used for language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b24c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. What is Multi-Head Attention?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb7e6fc",
   "metadata": {},
   "source": [
    "Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel. ... Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807acb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. What is Bilingual Evaluation Understudy (BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade24e20",
   "metadata": {},
   "source": [
    "\n",
    "BLEU, or the Bilingual Evaluation Understudy, is a score for comparing a candidate translation of text to one or more reference translations.\n",
    "\n",
    "Although developed for translation, it can be used to evaluate text generated for a suite of natural language processing tasks.\n",
    "\n",
    "In this tutorial, you will discover the BLEU score for evaluating and scoring candidate text using the NLTK library in Python.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "A gentle introduction to the BLEU score and an intuition for what is being calculated.\n",
    "How you can calculate BLEU scores in Python using the NLTK library for sentences and documents.\n",
    "How you can use a suite of small examples to develop an intuition for how differences between a candidate and reference text impact the final BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6dbe3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
