{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b95732",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What are Vanilla autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc995d",
   "metadata": {},
   "source": [
    "The vanilla autoencoder, as proposed by Hinton, consists of only one hidden layer. The number of neurons in the hidden layer is less than the number of neurons in the input (or output) layer. This results in producing a bottleneck effect on the flow of information in the network, and therefore we can think of the hidden layer as a bottleneck layer, restricting the information that would be stored. Learning in the autoencoder consists of developing a compact representation of the input signal at the hidden layer so that the output layer can faithfully reproduce the original input:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What are Sparse autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28115dc",
   "metadata": {},
   "source": [
    "A Sparse Autoencoder is a type of autoencoder that employs sparsity to achieve an information bottleneck. Specifically the loss function is constructed so that activations are penalized within a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db57bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What are Denoising autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec94fb1",
   "metadata": {},
   "source": [
    "Denoising autoencoders are an extension of the basic autoencoder, and represent a stochastic version of it. Denoising autoencoders attempt to address identity-function risk by randomly corrupting input (i.e. introducing noise) that the autoencoder must then reconstruct, or denoise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What are Convolutional autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4249bc0b",
   "metadata": {},
   "source": [
    "Convolutional Autoencoder is a variant of Convolutional Neural Networks that are used as the tools for unsupervised learning of convolution filters. They are generally applied in the task of image reconstruction to minimize reconstruction errors by learning the optimal filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f48add",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What are Stacked autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a86c392",
   "metadata": {},
   "source": [
    "A stacked autoencoder is a neural network consist several layers of sparse autoencoders where output of each hidden layer is connected to the input of the successive hidden layer. ... The learned data from the previous layer is used as an input for the next layer and this continues until the training is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5962f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Explain how to generate sentences using LSTM autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bcd1eb",
   "metadata": {},
   "source": [
    "An LSTM Autoencoder is an implementation of an autoencoder for sequence data using an Encoder-Decoder LSTM architecture.\n",
    "\n",
    "Once fit, the encoder part of the model can be used to encode or compress sequence data that in turn may be used in data visualizations or as a feature vector input to a supervised learning model.\n",
    "\n",
    "In this post, you will discover the LSTM Autoencoder model and how to implement it in Python using Keras.\n",
    "\n",
    "After reading this post, you will know:\n",
    "\n",
    "Autoencoders are a type of self-supervised learning model that can learn a compressed representation of input data.\n",
    "LSTM Autoencoders can learn a compressed representation of sequence data and have been used on video, text, audio, and time series sequence data.\n",
    "How to develop LSTM Autoencoder models in Python using the Keras deep learning library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f62ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Explain Extractive summarization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0525c9e",
   "metadata": {},
   "source": [
    "Extractive summarization aims at identifying the salient information that is then extracted and grouped together to form a concise summary. Abstractive summary generation rewrites the entire document by building internal semantic representation, and then a summary is created using natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13729c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Explain Abstractive summarization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1213ac65",
   "metadata": {},
   "source": [
    "Abstractive Summarization is a task in Natural Language Processing (NLP) that aims to generate a concise summary of a source text. ... Abstractive summarization yields a number of applications in different domains, from books and literature, to science and R&D, to financial research and legal documents analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Explain Beam search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03ceefc",
   "metadata": {},
   "source": [
    "Beam search is a heuristic search algorithm that explores a graph by expanding the most optimistic node in a limited set. Beam search is an optimization of best-first search that reduces its memory requirements.\n",
    "\n",
    "Best-first search is a graph search that orders all partial solutions according to some heuristic. But in beam search, only a predetermined number of best partial solutions are kept as candidates. Therefore, it is a greedy algorithm.\n",
    "\n",
    "Beam search uses breadth-first search to build its search tree. At each level of the tree, it generates all successors of the states at the current level, sorting them in increasing order of heuristic cost. However, it only stores a predetermined number (Î²), of best states at each level called the beamwidth. Only those states are expanded next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd524d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Explain Length normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032166dc",
   "metadata": {},
   "source": [
    "\n",
    "In the context of Vector Space model, cosine normalization adjusts the effect of document length on document weights by computing the cosine similarity between the query and the document weight vectors. ... This normalization function is derived from the study of the document length effect in the 2-Poisson model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e1018",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. Explain Coverage normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. Explain ROUGE metric evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9d830c",
   "metadata": {},
   "source": [
    "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It is essentially a set of metrics for evaluating automatic summarization of texts as well as machine translations. It works by comparing an automatically produced summary or translation against a set of reference summaries (typically human-produced)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
