{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Explain the architecture of BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2f38cc",
   "metadata": {},
   "source": [
    "BERT, which stands for Bidirectional Encoder Representations from Transformers, is based on Transformers, a deep learning model in which every output element is connected to every input element, and the weightings between them are dynamically calculated based upon their connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Explain Masked Language Modeling (MLM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2656c5",
   "metadata": {},
   "source": [
    "\n",
    "Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Explain Next Sentence Prediction (NSP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39b7b2",
   "metadata": {},
   "source": [
    "In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f6f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What is Matthews evaluation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bfba52",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What is Matthews Correlation Coefficient (MCC)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535b4e97",
   "metadata": {},
   "source": [
    "The Matthews Correlation Coefficient (MCC) is one of the popular measurements for classification accuracy. It has been generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The study of this paper finds that this is not true. MCC deteriorates seriously when the dataset in classification are imbalanced. Experiment results and analysis show that MCC is not suitable for classification accuracy measurement on imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6993eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Explain Semantic Role Labeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba10ea8",
   "metadata": {},
   "source": [
    "A semantic role is the underlying relationship that a participant has with the main verb in a clause. Discussion: Semantic role is the actual role a participant plays in some real or imagined situation, apart from the linguistic encoding of those situations. ... Agent As A Semantic Role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c5143",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Why Fine-tuning a BERT model takes less time than pretraining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f419e1",
   "metadata": {},
   "source": [
    "While finetuning BERT is relatively straightforward in theory, it can be time-intensive and unrewarding in practice due to seemingly random outcomes of different training runs. In fact, even when finetuning a model with the same hyperparameters over and over again, there can be a great degree of variability in final model performance due to randomness in (1) weight intializations and (2) data orders (how data sets are shuffled). This is especially a problem when finetuning BERT on small data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Recognizing Textual Entailment (RTE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c47e864",
   "metadata": {},
   "source": [
    "Recognizing Textual Entailment (RTE) was proposed as a unified evaluation framework to compare semantic understanding of different NLP systems. In this survey paper, we provide an overview of different approaches for evaluating and understanding the reasoning capabilities of NLP systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Explain the decoder stack of GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f41560",
   "metadata": {},
   "source": [
    "This year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de42202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2e1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
